#### Basic SALT commands ####

Get all documentation: 
 :~ # salt $salt_minion sys.doc

Get specific documentation: 
 :~ # salt $salt_minion sys.doc $item
Example: 
 :~ # salt ses1 sys.doc grains.setval

List functions:
 :~ #  salt ‘*’ sys.list_functions

List all grains:
 :~ #  salt ‘*’ grains.items

Check specific grain:
 :~ #  salt ‘*’ grains.item $item_name

Check service status: 
 :~ # salt ‘*’ service.status $service_name

Set grain value(s): 
 :~ # salt ‘*’ grain.setval(s) $key_name $value

Dry run check/verify state: 
 :~ # salt $target state.sls $state_name test=True

Apply specific function from a state: 
 :~ # salt-call state.single fun=$function ???

Apply the highstate limited to 10% of total minions:
 :~ # salt \* --batch-size 10% state.highstate 
	OR to only 20 at a time: 
 :~ # salt \* -b 20 state.highstate

List active jobs:
 :~ # salt-run jobs.active

Kill a stuck job:
 :~ # salt 'ins_minion*' saltutil.kill_job <job_id>

List job history:
 :~ # salt-run jobs.list_jobs

Refresh the pillar:
 :~ # salt \* saltutil.refresh_pillar

Show pillar information:
 :~ # salt -I 'cluster:ceph' pillar.items

#### Targeting Minions ####

Using Pillar data:
 e.g. Only MONs:
 :~ # salt -I ‘roles:mon’ test.ping
 e.g. all cluster nodes in pillar:
 :~ # salt -I 'cluster:ceph' test.ping

Regular Expression:
 :~ # salt -E ‘ses-nue-[1,2,3].*’

#### DeepSea Specific ####

Verify expected processes:
 :~ # salt \* cephprocesses.wait
 If False is returned for some minions, additionally verify which processes:
 :~ # salt 'false_minion*' cephprocesses.check results=True

Stage 2 osd.advise error "TypeError: string indices must be integers, not str"
 Make sure grains are up to date:
 :~ # salt -I 'roles:storage' osd.retain | tee retain.out
 Verify status:
 :~ # salt -I 'roles:storage' pillar.get ceph | tee pillar.out
 :~ # salt -I 'roles:storage' grains.get ceph | tee grains.out
 :~ # salt -I 'roles:storage' osd.report human=False | tee osd-report.out

Verify the "iscsi.wait" module function:
 :~ # salt 'ins_igw_minion*' service.status rbd-target-api.service
 :~ # salt 'ins_igw_minion*' http.query http(s)://<usr>:<pass>@ins_igw_dns:5000/api/_ping verify=False timeout=5

Get ceph-disk commands (only SES5x):

 :~ # salt 'ins_minion*' osd.prepare /dev/sd<x>
 :~ # salt 'ins_minion*' osd.activate /dev/sd<x>

Create and push out only a new ceph.conf:

 :~ # salt 'ins_admin*' state.apply ceph.configuration.create
 :~ # salt -I 'cluster:ceph' state.apply ceph.configuration

Create custom static RGW entries, can be for any custom ceph.conf entry to prevent multiple entry duplication:
!!WARNING!! - this is likely incorrect!!
 :~ # vim /srv/salt/ceph/configuration/files/ceph.conf.d/rgw.conf
 Add custom client RGW entries
 :~ # cp /srv/salt/ceph/configuration/create/default.sls /srv/salt/ceph/configuration/create/custom.sls
 :~ # vim /srv/salt/ceph/configuration/create/custom.sls
 Change the line:
    - source: salt://ceph/configuration/files/custom-ceph.conf.j2
 Thus "ceph.conf.j2" to "custom-ceph.conf.j2"
 :~ # cp /srv/salt/ceph/configuration/files/ceph.conf.j2 /srv/salt/ceph/configuration/files/custom-ceph.conf.j2
 Edit the custom-ceph.conf.j2 and remark the following lines:

 ------
 #{% for config,host,fqdn in salt.saltutil.runner('select.from', pillar='rgw_configurations', role='rgw', attr='host, fqdn') %}
 ##{% if config is not none %}
 ##{% set client = config + "." + host %}
 ------

 :~ # vim /srv/pillar/ceph/stack/global.yml
 Add the line:
 configuration_create: custom
 :~ # salt '*' saltutil.refresh_pillar
 Create and redeploy the ceph.conf from the admin node.
