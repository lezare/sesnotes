1. Pool relevant commands to note:

- ceph osd pool get pool_name setting_name
eg. ceph osd pool get cachessd hit_set_fpp
- rados -p cache_pool ls (make sure pool is flushed)
- rados -p cache_pool cache-flush-evict-all (flush all objects manually)

2. Create Erasure Coded Pool:

* NOTE: EC Pools can NOT be used as cache tiers!

Display defaults: ceph osd erasure-code-profile get default

* NOTE: Profiles are stored in "/usr/lib64/ceph/erasure-code/"

Create custom profile: ceph osd erasure-code-profile set new_profile_name k=3 m=2 
Create ecpool: ceph osd pool create pool_name pg pgn erasure ec_profile_name rule_name

Example: new profile name=eccustom / pool_name=spinners / rule_name=spinners / PG=100:

ceph osd erasure-code-profile set eccustom k=3 m=2 
ceph osd pool create spinners 100 100 erasure eccustom spinners

3. Create Replicated (standard) Pool:

ceph osd pool create ssd 100 100 replicated ssd

4. Create Cache Tier:

* NOTE: My LAB: spinners = storage_pool / cache_pool = ssd

ceph osd tier add storage_pool cahce_pool
ceph osd tier cache-mode cache_pool writeback
ceph osd tier set-overlay storage_pool cache_pool

5. Configure cache tier settings, these are NOT set by default after configuring a cache tier:

ceph osd pool set ssd hit_set_type bloom
ceph osd pool set ssd hit_set_count 1 (Default will be 0)
ceph osd pool set ssd hit_set_period 3600 (Default will be 0)
ceph osd pool set ssd target_max_bytes 1000000000000 (Default will be 0)
ceph osd pool set {cachepool} min_read_recency_for_promote 1 (Default will be 0)
ceph osd pool set {cachepool} min_write_recency_for_promote 1 (only available from "Infernalis" version) (Default will be 0)
ceph osd pool set hot-storage cache_target_dirty_ratio 0.4 (Flush when dirty objects reach 40% of cache pool capacity) (Default will be 400000)
ceph osd pool set hot-storage cache_target_dirty_high_ratio 0.6 (Aggresively start flushing at 60% - only available from "Infernalis" version) (Default will be 600000)
ceph osd pool set hot-storage cache_target_full_ratio 0.8 (Evict or flush clean / unmodified object at 80% of cache pool capacity) (Default will be 800000)

# NOTE: The last 3 options above with SES 3 onwards will be specified with "_micro" added eg. "cache_target_dirty_ratio_micro"

6. Delete:

ceph osd pool delete pool_name pool_name_again â€“yes-i-really-really-mean-it

6a. WRITEBACK CACHE:

ceph osd tier cache-mode {cachepool} forward (change to forward)

rados -p {cachepool} ls (make sure pool is flushed)
rados -p {cachepool} cache-flush-evict-all (flush all objects manually)
ceph osd tier remove-overlay {storagetier} (remove overly from cold_storage)
ceph osd tier remove {storagepool} {cachepool} (remove cache tier pool from the backing storagepool)

7. Increase or set Placement Groups:

ceph osd pool set pool_name pg_num ins_value (Placement Group_number)
ceph osd pool set pool_name pgp_num ins_value (Placement Group for Placement_number)

May need to add "--yes-i-really-mean-it" if the pool is a cache pool.
